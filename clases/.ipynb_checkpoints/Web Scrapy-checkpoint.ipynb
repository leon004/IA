{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapy \n",
    "\n",
    "Web scraping es una técnica utilizada mediante programas de software para extraer información de sitios web. Usualmente, estos programas simulan la navegación de un humano en la World Wide Web ya sea utilizando el protocolo HTTP manualmente, o incrustando un navegador en una aplicación.\n",
    "\n",
    "\n",
    "El web scraping está muy relacionado con la indexación de la web, la cual indexa la información de la web utilizando un robot y es una técnica universal adoptada por la mayoría de los motores de búsqueda. Sin embargo, el web scraping se enfoca más en la transformación de datos sin estructura en la web (como el formato HTML) en datos estructurados que pueden ser almacenados y analizados en una base de datos central, en una hoja de cálculo o en alguna otra fuente de almacenamiento. Alguno de los usos del web scraping son la comparación de precios en tiendas, la monitorización de datos relacionados con el clima de cierta región, la detección de cambios en sitios webs y la integración de datos en sitios webs. También es utilizado para obtener información relevante de un sitio a través de los rich snippets.\n",
    "\n",
    "\n",
    "### Arquitectura Scrapy\n",
    "La Arquitectura de un web Scraping es muy simple:\n",
    "\n",
    "<img src=\"sc1.png\">\n",
    "\n",
    "El primer paso será crear el “Engine”, el cual obtiene las “Requests” iniciales que los “Spiders” le envían.\n",
    "\n",
    "El “Engine” programa las “Requests” en el planificador y solicita las siguientes “Requests” que se deben rastrear (crawl).\n",
    "\n",
    "El Programador o “Scheduler” devuelve las próximas “Requests” al “ Engine”.\n",
    "\n",
    "El “Engine” envía las “Requests” al “Downloader”, pasando por los diferentes “Middlewares” del “Downloader”.\n",
    "\n",
    "Bien, los anteriores pasos son cuando, requerimos al sistema de un proceso de descarga de información, entonces cuando la página termina de descargar el contenido que le hemos solicitado, el “Downloader” genera una Respuesta con dicho contenido y la envía al “Engine”, pasando de nuevo por los “Middlewares” del “Downloader”. Cuando el “Engine” recibe la respuesta del “Downloader”, la envía a los “Spiders”, de nuevo, para que sea procesada la información.\n",
    "\n",
    "Los “Spiders” procesan la respuestas y son devueltos los elementos identificados (scraped) y las nuevas “Requests” al “Engine”, pasando a través de los Middleware de los “Spiders”.\n",
    "El “Engine” envía los elementos procesados a las “Pipeline” de elementos, luego envía las solicitudes procesadas al “Scheduler” y pide posibles próximas solicitudes para rastrear.\n",
    "El proceso se repite, desde el paso 1, hasta que no haya más solicitudes del “Scheduler”.\n",
    "El proceso, como tal, no es complejo pero es recurrente. En el anterior diagrama se muestra una visión general de la arquitectura Scrapy con sus componentes y un esquema del flujo de datos que tiene lugar dentro del sistema. Que seguramente ayuda y mucho a interpretar el texto de explicación.\n",
    "\n",
    "\n",
    "### Intalación en python \n",
    "\n",
    "pip install scrapy \n",
    "\n",
    "\n",
    "### Configuracion del Path \n",
    "\n",
    "export PATH=\"${PATH}:${HOME}/.local/bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: scrapy: not found\r\n"
     ]
    }
   ],
   "source": [
    "#Este comando nos permite crear un proyecto de scrapy dentro \n",
    "#de nuestra carpeta de jupyter \n",
    "!scrapy startproject prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/likcos/Programacion/Notebooks/prueba\n"
     ]
    }
   ],
   "source": [
    "# para explorar el proyecto es necesario teclear el siguiente comando\n",
    "# de igual forma es conveniete cambiar la ruta, path a su maquina personal \n",
    "\n",
    "%cd prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/likcos/Programacion/Notebooks/prueba'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mprueba\u001b[00m\r\n",
      "│   ├── __init__.py\r\n",
      "│   ├── items.py\r\n",
      "│   ├── middlewares.py\r\n",
      "│   ├── pipelines.py\r\n",
      "│   ├── settings.py\r\n",
      "│   └── \u001b[01;34mspiders\u001b[00m\r\n",
      "│       └── __init__.py\r\n",
      "└── scrapy.cfg\r\n",
      "\r\n",
      "2 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo \n",
    "\n",
    "El uso de herramientas automatizadas para la adquisición de contenido web (web crawling) es una práctica que en algunos casos no es aceptable para ciertos sitios en internet y posible que se bloquie la ip desde donte se intenta obtener la información.\n",
    "\n",
    "En la estructura anterior se puede apreciar que la carpeta spider esta bacía debido a que no sea generado ningun spider para obtener la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'ejemplo' using template 'basic' in module:\r\n",
      "  prueba.spiders.ejemplo\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider ejemplo itmorelia.edu.mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mprueba\u001b[00m\r\n",
      "│   ├── __init__.py\r\n",
      "│   ├── __init__.pyc\r\n",
      "│   ├── items.py\r\n",
      "│   ├── middlewares.py\r\n",
      "│   ├── pipelines.py\r\n",
      "│   ├── settings.py\r\n",
      "│   ├── settings.pyc\r\n",
      "│   └── \u001b[01;34mspiders\u001b[00m\r\n",
      "│       ├── ejemplo.py\r\n",
      "│       ├── __init__.py\r\n",
      "│       └── __init__.pyc\r\n",
      "└── scrapy.cfg\r\n",
      "\r\n",
      "2 directories, 11 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\r\n",
      "import scrapy\r\n",
      "\r\n",
      "\r\n",
      "class EjemploSpider(scrapy.Spider):\r\n",
      "    name = 'ejemplo'\r\n",
      "    allowed_domains = ['itmorelia.edu.mx']\r\n",
      "    start_urls = ['http://itmorelia.edu.mx/']\r\n",
      "\r\n",
      "    def parse(self, response):\r\n",
      "        pass\r\n"
     ]
    }
   ],
   "source": [
    "%cat prueba/spiders/ejemplo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scrapy.spiders in scrapy:\n",
      "\n",
      "NAME\n",
      "    scrapy.spiders - Base class for Scrapy spiders\n",
      "\n",
      "DESCRIPTION\n",
      "    See documentation in docs/topics/spiders.rst\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    crawl\n",
      "    feed\n",
      "    init\n",
      "    sitemap\n",
      "\n",
      "CLASSES\n",
      "    scrapy.utils.trackref.object_ref(builtins.object)\n",
      "        Spider\n",
      "    \n",
      "    class Spider(scrapy.utils.trackref.object_ref)\n",
      "     |  Spider(*args, **kwargs)\n",
      "     |  \n",
      "     |  Base class for scrapy spiders. All spiders must inherit from this\n",
      "     |  class.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Spider\n",
      "     |      scrapy.utils.trackref.object_ref\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, name=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__ = __str__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  log(self, message, level=10, **kw)\n",
      "     |      Log the given message at the given log level\n",
      "     |      \n",
      "     |      This helper wraps a log call to the logger within the spider, but you\n",
      "     |      can use it directly (e.g. Spider.logger.info('msg')) or use any other\n",
      "     |      Python logger too.\n",
      "     |  \n",
      "     |  make_requests_from_url(self, url)\n",
      "     |      This method is deprecated.\n",
      "     |  \n",
      "     |  parse(self, response)\n",
      "     |  \n",
      "     |  start_requests(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_crawler(crawler, *args, **kwargs) from builtins.type\n",
      "     |  \n",
      "     |  handles_request(request) from builtins.type\n",
      "     |  \n",
      "     |  update_settings(settings) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  close(spider, reason)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  logger\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  custom_settings = None\n",
      "     |  \n",
      "     |  name = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from scrapy.utils.trackref.object_ref:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwargs)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n",
      "FILE\n",
      "    /home/likcos/.local/lib/python3.7/site-packages/scrapy/spiders/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(scrapy.spiders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La clase scrapy.Spider.\n",
    "Todas las arañas de scrapy son subclases de la clase scrapy.Spider, la cual define por defecto los siguientes atributos y métodos:\n",
    "\n",
    "- El atributo name, el cual es el nombre que se el asignará a cada objeto instanciado.\n",
    "\n",
    "- allowed_domains, que se refiere a un objeto tipo list que contiene la lista de los dominios a los que puede acceder la araña.\n",
    "\n",
    "- El atributo start_urls, que corresponde a una lista de URLs a partir de los cuales las arañas escudriñarán el contenido.\n",
    "\n",
    "- El método parse(), el cual en un principio es un método abstracto y se utiliza para definir los contenidos que debe de buscar la araña.\n",
    "\n",
    "- El método start_requests() se encarga de iniciar la carga de datos a partir de los URL ingresados en start_urls y obteniendo los datos definidos en parse. El resultado es un objeto llamado Request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-08 22:57:17 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: prueba)\n",
      "2019-10-08 22:57:17 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.4, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 2.7.16 (default, Apr  6 2019, 01:42:57) - [GCC 8.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.6.1, Platform Linux-4.19.0-6-amd64-x86_64-with-debian-10.1\n",
      "2019-10-08 22:57:17 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'prueba.spiders', 'SPIDER_MODULES': ['prueba.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'prueba'}\n",
      "2019-10-08 22:57:17 [scrapy.extensions.telnet] INFO: Telnet Password: b83bc9b61eebf4b1\n",
      "2019-10-08 22:57:17 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2019-10-08 22:57:17 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-10-08 22:57:17 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-10-08 22:57:17 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-10-08 22:57:17 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-10-08 22:57:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-10-08 22:57:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-10-08 22:57:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://itmorelia.edu.mx/robots.txt> (referer: None)\n",
      "2019-10-08 22:57:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://itmorelia.edu.mx/> (referer: None)\n",
      "2019-10-08 22:57:17 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-10-08 22:57:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 440,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 3206,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 0.226709,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 10, 9, 3, 57, 17, 788603),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 54689792,\n",
      " 'memusage/startup': 54689792,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/200': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2019, 10, 9, 3, 57, 17, 561894)}\n",
      "2019-10-08 22:57:17 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mprueba\u001b[00m\r\n",
      "│   ├── __init__.py\r\n",
      "│   ├── __init__.pyc\r\n",
      "│   ├── items.py\r\n",
      "│   ├── middlewares.py\r\n",
      "│   ├── pipelines.py\r\n",
      "│   ├── settings.py\r\n",
      "│   ├── settings.pyc\r\n",
      "│   └── \u001b[01;34mspiders\u001b[00m\r\n",
      "│       ├── ejemplo.py\r\n",
      "│       ├── ejemplo.pyc\r\n",
      "│       ├── __init__.py\r\n",
      "│       └── __init__.pyc\r\n",
      "└── scrapy.cfg\r\n",
      "\r\n",
      "2 directories, 12 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-08 23:04:16 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: prueba)\n",
      "2019-10-08 23:04:16 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.4, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 2.7.16 (default, Apr  6 2019, 01:42:57) - [GCC 8.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.6.1, Platform Linux-4.19.0-6-amd64-x86_64-with-debian-10.1\n",
      "2019-10-08 23:04:16 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'prueba.spiders', 'SPIDER_MODULES': ['prueba.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'prueba'}\n",
      "2019-10-08 23:04:16 [scrapy.extensions.telnet] INFO: Telnet Password: 6c104554506d1ed3\n",
      "2019-10-08 23:04:16 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2019-10-08 23:04:16 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-10-08 23:04:16 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-10-08 23:04:16 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-10-08 23:04:16 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-10-08 23:04:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-10-08 23:04:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-10-08 23:04:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://itmorelia.edu.mx/robots.txt> (referer: None)\n",
      "2019-10-08 23:04:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://itmorelia.edu.mx/> (referer: None)\n",
      "<!doctype html><html lang=\"es\"><head><!-- Global site tag (gtag.js) - Google Analytics --><script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-110355574-1\"></script><script>window.dataLayer = window.dataLayer || [];\n",
      "    function gtag(){dataLayer.push(arguments);}\n",
      "    gtag('js', new Date());\n",
      "    gtag('config', 'UA-110355574-1');</script><meta charset=\"utf-8\"><title>ITM</title><base href=\"/\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><!--link rel=\"icon\" type=\"image/x-icon\" href=\"favicon.ico\"--><!-- CSS --><link href=\"/favicon.png\" rel=\"shortcut icon\"><link rel=\"stylesheet\" type=\"text/css\" href=\"assets/Icons/font/Footer/flaticon.css\"><!--<link href=\"https://framework-gb.cdn.gob.mx/assets/styles/main.css\" rel=\"stylesheet\">--><link rel=\"stylesheet\" type=\"text/css\" href=\"bootstrap.css\"><!--link href=\"https://fonts.googleapis.com/css?family=Courgette\" rel=\"stylesheet\"--><style>@media (min-width: 992px){\n",
      "        .container {\n",
      "            width: 900px;\n",
      "        }\n",
      "      }\n",
      "\n",
      "      @media (min-width: 768px){\n",
      "        .container {\n",
      "            width: 700px;\n",
      "        }\n",
      "      }\n",
      "\n",
      "      @media (min-width: 1200px){\n",
      "        .container {\n",
      "            width: 1000px;\n",
      "        }\n",
      "      }\n",
      "      .alert.alert-warning.warning-nav{\n",
      "        display: none;\n",
      "      }</style><!-- Respond.js soporte de media queries para Internet Explorer 8 --><!-- ie8.js EventTarget para cada nodo en Internet Explorer 8 --><!--[if lt IE 9]>\n",
      "      <script src=\"https://oss.maxcdn.com/respond/1.4.2/respond.min.js\"></script>\n",
      "      <script src=\"https://cdnjs.cloudflare.com/ajax/libs/ie8/0.2.2/ie8.js\"></script>\n",
      "    <![endif]--><link href=\"styles.ed77fcbaa1ffcdabf1f0.bundle.css\" rel=\"stylesheet\"/></head><body><script src=\"https://framework-gb.cdn.gob.mx/gobmx.js\"></script><script type=\"text/javascript\">function googleTranslateElementInit() {\n",
      "      new google.translate.TranslateElement({\n",
      "        pageLanguage: 'es',\n",
      "        layout: google.translate.TranslateElement.InlineLayout.VERTICAL},\n",
      "      'google_translate_element');\n",
      "    }</script><script type=\"text/javascript\" src=\"//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit\"></script><app-root></app-root><!-- JS --><script>$gmx(document).ready(function() {\n",
      "      setTimeout(function(){\n",
      "        //console.log(\"Gob mx cargado\");\n",
      "        /*\n",
      "        let alto = $('#menu-tec').height();\n",
      "        $('#google_translate_element').attr('style',  'margin-top: '+alto+'px');\n",
      "      }, 500);\n",
      "      $(window).on(\"resize\", function(){\n",
      "        let alto = $('#menu-tec').height();\n",
      "        $('#google_translate_element').attr('style',  'margin-top: '+alto+'px');\n",
      "      });\n",
      "      */\n",
      "      $('.carousel').carousel({\n",
      "        interval: 5000,\n",
      "        pause: \"hover\"\n",
      "      })\n",
      "    });</script><script type=\"text/javascript\" src=\"inline.747dcc9eff348b4c25b6.bundle.js\"></script><script type=\"text/javascript\" src=\"polyfills.e790f33de8542e1a52f1.bundle.js\"></script><script type=\"text/javascript\" src=\"scripts.60d8305e41411aead71c.bundle.js\"></script><script type=\"text/javascript\" src=\"vendor.80712accb22ae5194c2a.bundle.js\"></script><script type=\"text/javascript\" src=\"main.a6f4bb3ba28e5830b62e.bundle.js\"></script></body></html>\n",
      "2019-10-08 23:04:18 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-10-08 23:04:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 440,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 3206,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 1.253024,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 10, 9, 4, 4, 18, 93769),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 54833152,\n",
      " 'memusage/startup': 54833152,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/200': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2019, 10, 9, 4, 4, 16, 840745)}\n",
      "2019-10-08 23:04:18 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy fetch --spider=ejemplo http://itmorelia.edu.mx/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selectores. \n",
    "\n",
    "Los selectores son los objetos que se encargan de realizar búsquedas específicas dentro de un texto estructurado y son instancias del objetos scrapy.Selector.\n",
    "\n",
    "La clase scrapy.Selector contiene varios métodos, entre los que se incluyen:\n",
    "\n",
    "- css(), permite realizar búsqueda de nodos dentro de un documento HTML mediante la sintaxis de selectores de CSS.\n",
    "\n",
    "- xpath(), permite realizar búsquedas de nodos dentro de un documento HTML/XML mediante XPath.\n",
    "\n",
    "- re(), permite realizar búsquedas de nodos dentro de un documento HTML/XML mediante expresiones regulares.\n",
    "\n",
    "- extract() regresa los datos correspondientes a un nodo encontrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b5a722e83d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrapy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scrapy' is not defined"
     ]
    }
   ],
   "source": [
    "help(scrapy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El shell de scrapy.\n",
    "Scrapy cuenta con su propio entorno interactivo, el cual es muy similar al shell de Python. Este entorno nos permite probar las búsquedas que se pueden realizar en un sitio específico.\n",
    "\n",
    "Este shell permite al usuario interactuar con los objetos y elementos de un proyecto.\n",
    "\n",
    "Lo único que se requiere es ejecutar lo siguiente desde la línea de comando:\n",
    "\n",
    "scrapy shell <URL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-13 23:39:29 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
      "2019-10-13 23:39:29 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.4, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 2.7.16 (default, Apr  6 2019, 01:42:57) - [GCC 8.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.6.1, Platform Linux-4.19.0-6-amd64-x86_64-with-debian-10.1\n",
      "2019-10-13 23:39:29 [scrapy.crawler] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n",
      "2019-10-13 23:39:29 [scrapy.extensions.telnet] INFO: Telnet Password: fd2f8994e6730c8b\n",
      "2019-10-13 23:39:29 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2019-10-13 23:39:29 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-10-13 23:39:29 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-10-13 23:39:29 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-10-13 23:39:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-10-13 23:39:29 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-10-13 23:39:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://likcos.blogspot.com/> (referer: None)\n",
      "[s] Available Scrapy objects:\n",
      "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
      "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f18804adcd0>\n",
      "[s]   item       {}\n",
      "[s]   request    <GET http://likcos.blogspot.com/>\n",
      "[s]   response   <200 http://likcos.blogspot.com/>\n",
      "[s]   settings   <scrapy.settings.Settings object at 0x7f18804adbd0>\n",
      "[s]   spider     <DefaultSpider 'default' at 0x7f187f80e410>\n",
      "[s] Useful shortcuts:\n",
      "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
      "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
      "[s]   shelp()           Shell help (print this help)\n",
      "[s]   view(response)    View response in a browser\n",
      ">>> \n",
      ">>> "
     ]
    }
   ],
   "source": [
    "!scrapy shell http://likcos.blogspot.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-17ac3bc2e51d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//h3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response.xpath('//h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-437d4e411f2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//dt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "for item in response.xpath('//dt'):\n",
    "    print(item.extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in response.css('.precio'):\n",
    "    print(item.extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'prueba.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0e3b66e98d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prueba.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prueba.json'"
     ]
    }
   ],
   "source": [
    "with open('prueba.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
